{
    "title": "Amazon Kinesis Firehose",
    "description": "Amazon Kinesis Data Firehose API Reference Amazon Kinesis Data Firehose is a\nfully managed service that delivers real-time streaming data to destinations\nsuch as Amazon Simple Storage Service (Amazon S3), Amazon Elasticsearch Service\n(Amazon ES), Amazon Redshift, and Splunk.",
    "docsUrl": "https://aws.amazon.com/firehose/",
    "url": "https://api.apis.guru/v2/specs/amazonaws.com/firehose/2015-08-04/swagger.json",
    "envVars": {},
    "credentials": {
        "fields": {
            "server": {
                "label": "Server",
                "viewClass": "SelectView",
                "model": [
                    "https://firehose.amazonaws.com/",
                    "http://firehose.amazonaws.com/",
                    "--- Custom URL"
                ],
                "required": true
            },
            "otherServer": {
                "label": "Custom Server URL",
                "viewClass": "TextFieldView"
            },
            "auth_hmac": {
                "label": "Authorization (hmac)",
                "viewClass": "TextFieldView",
                "note": "Amazon Signature authorization v4"
            }
        }
    },
    "triggers": {
        "startFlow": {
            "main": "./lib/triggers/startFlow.js",
            "type": "polling",
            "title": "Start Flow",
            "fields": {}
        }
    },
    "actions": {
        "CreateDeliveryStream": {
            "main": "./lib/actions/CreateDeliveryStream.js",
            "title": "CreateDeliveryStream",
            "description": "Creates a Kinesis Data Firehose delivery stream.\n\nBy default, you can create up to 50 delivery streams per AWS Region.\n\nThis is an asynchronous operation that immediately returns. The initial status\nof the delivery stream is CREATING. After the delivery stream is created, its\nstatus is ACTIVE and it now accepts data. Attempts to send data to a delivery\nstream that is not in the ACTIVE state cause an exception. To check the state of\na delivery stream, use DescribeDeliveryStream.\n\nA Kinesis Data Firehose delivery stream can be configured to receive records\ndirectly from providers using PutRecord or PutRecordBatch, or it can be\nconfigured to use an existing Kinesis stream as its source. To specify a Kinesis\ndata stream as input, set the DeliveryStreamType parameter to \nKinesisStreamAsSource, and provide the Kinesis stream Amazon Resource Name (ARN)\nand role ARN in the KinesisStreamSourceConfiguration parameter.\n\nA delivery stream is configured with a single destination: Amazon S3, Amazon ES,\nAmazon Redshift, or Splunk. You must specify only one of the following\ndestination configuration parameters: ExtendedS3DestinationConfiguration, \nS3DestinationConfiguration, ElasticsearchDestinationConfiguration, \nRedshiftDestinationConfiguration, or SplunkDestinationConfiguration.\n\nWhen you specify S3DestinationConfiguration, you can also provide the following\noptional values: BufferingHints, EncryptionConfiguration, and CompressionFormat.\nBy default, if no BufferingHints value is provided, Kinesis Data Firehose\nbuffers data up to 5 MB or for 5 minutes, whichever condition is satisfied\nfirst. BufferingHints is a hint, so there are some cases where the service\ncannot adhere to these conditions strictly. For example, record boundaries might\nbe such that the size is a little over or under the configured buffering size.\nBy default, no encryption is performed. We strongly recommend that you enable\nencryption to ensure secure data storage in Amazon S3.\n\nA few notes about Amazon Redshift as a destination:\n\n *  An Amazon Redshift destination requires an S3 bucket as intermediate\n   location. Kinesis Data Firehose first delivers data to Amazon S3 and then\n   uses COPY syntax to load data into an Amazon Redshift table. This is\n   specified in the RedshiftDestinationConfiguration.S3Configuration parameter.\n   \n   \n *  The compression formats SNAPPY or ZIP cannot be specified in \n   RedshiftDestinationConfiguration.S3Configuration because the Amazon Redshift \n   COPY operation that reads from the S3 bucket doesn't support these\n   compression formats.\n   \n   \n *  We strongly recommend that you use the user name and password you provide\n   exclusively with Kinesis Data Firehose, and that the permissions for the\n   account are restricted for Amazon Redshift INSERT permissions.\n   \n   \n\nKinesis Data Firehose assumes the IAM role that is configured as part of the\ndestination. The role should allow the Kinesis Data Firehose principal to assume\nthe role, and the role should have permissions that allow the service to deliver\nthe data. For more information, see Grant Kinesis Data Firehose Access to an\nAmazon S3 Destination\n[http://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3] \nin the Amazon Kinesis Data Firehose Developer Guide.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/CreateDeliveryStream.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DeleteDeliveryStream": {
            "main": "./lib/actions/DeleteDeliveryStream.js",
            "title": "DeleteDeliveryStream",
            "description": "Deletes a delivery stream and its data.\n\nYou can delete a delivery stream only if it is in ACTIVE or DELETING state, and\nnot in the CREATING state. While the deletion request is in process, the\ndelivery stream is in the DELETING state.\n\nTo check the state of a delivery stream, use DescribeDeliveryStream.\n\nWhile the delivery stream is DELETING state, the service might continue to\naccept the records, but it doesn't make any guarantees with respect to\ndelivering the data. Therefore, as a best practice, you should first stop any\napplications that are sending records before deleting a delivery stream.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DeleteDeliveryStream.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DescribeDeliveryStream": {
            "main": "./lib/actions/DescribeDeliveryStream.js",
            "title": "DescribeDeliveryStream",
            "description": "Describes the specified delivery stream and gets the status. For example, after your delivery stream is created, call <code>DescribeDeliveryStream</code> to see whether the delivery stream is <code>ACTIVE</code> and therefore ready for data to be sent to it.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DescribeDeliveryStream.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "ListDeliveryStreams": {
            "main": "./lib/actions/ListDeliveryStreams.js",
            "title": "ListDeliveryStreams",
            "description": "Lists your delivery streams in alphabetical order of their names.\n\nThe number of delivery streams might be too large to return using a single call\nto ListDeliveryStreams. You can limit the number of delivery streams returned,\nusing the Limit parameter. To determine whether there are more delivery streams\nto list, check the value of HasMoreDeliveryStreams in the output. If there are\nmore delivery streams to list, you can request them by calling this operation\nagain and setting the ExclusiveStartDeliveryStreamName parameter to the name of\nthe last delivery stream returned in the last call.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/ListDeliveryStreams.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "ListTagsForDeliveryStream": {
            "main": "./lib/actions/ListTagsForDeliveryStream.js",
            "title": "ListTagsForDeliveryStream",
            "description": "Lists the tags for the specified delivery stream. This operation has a limit of five transactions per second per account. ",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/ListTagsForDeliveryStream.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "PutRecord": {
            "main": "./lib/actions/PutRecord.js",
            "title": "PutRecord",
            "description": "Writes a single data record into an Amazon Kinesis Data Firehose delivery\nstream. To write multiple data records into a delivery stream, use \nPutRecordBatch. Applications using these operations are referred to as\nproducers.\n\nBy default, each delivery stream can take in up to 2,000 transactions per\nsecond, 5,000 records per second, or 5 MB per second. If you use PutRecord and \nPutRecordBatch, the limits are an aggregate across these two operations for each\ndelivery stream. For more information about limits and how to request an\nincrease, see Amazon Kinesis Data Firehose Limits\n[http://docs.aws.amazon.com/firehose/latest/dev/limits.html]. \n\nYou must specify the name of the delivery stream and the data record when using \nPutRecord. The data record consists of a data blob that can be up to 1,000 KB in\nsize, and any kind of data. For example, it can be a segment from a log file,\ngeographic location data, website clickstream data, and so on.\n\nKinesis Data Firehose buffers records before delivering them to the destination.\nTo disambiguate the data blobs at the destination, a common solution is to use\ndelimiters in the data, such as a newline (\\n) or some other character unique\nwithin the data. This allows the consumer application to parse individual data\nitems when reading the data from the destination.\n\nThe PutRecord operation returns a RecordId, which is a unique string assigned to\neach record. Producer applications can use this ID for purposes such as\nauditability and investigation.\n\nIf the PutRecord operation throws a ServiceUnavailableException, back off and\nretry. If the exception persists, it is possible that the throughput limits have\nbeen exceeded for the delivery stream. \n\nData records sent to Kinesis Data Firehose are stored for 24 hours from the time\nthey are added to a delivery stream as it tries to send the records to the\ndestination. If the destination is unreachable for more than 24 hours, the data\nis no longer available.\n\nDon't concatenate two or more base64 strings to form the data fields of your\nrecords. Instead, concatenate the raw data, then perform base64 encoding.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/PutRecord.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "PutRecordBatch": {
            "main": "./lib/actions/PutRecordBatch.js",
            "title": "PutRecordBatch",
            "description": "Writes multiple data records into a delivery stream in a single call, which can\nachieve higher throughput per producer than when writing single records. To\nwrite single data records into a delivery stream, use PutRecord. Applications\nusing these operations are referred to as producers.\n\nBy default, each delivery stream can take in up to 2,000 transactions per\nsecond, 5,000 records per second, or 5 MB per second. If you use PutRecord and \nPutRecordBatch, the limits are an aggregate across these two operations for each\ndelivery stream. For more information about limits, see Amazon Kinesis Data\nFirehose Limits [http://docs.aws.amazon.com/firehose/latest/dev/limits.html].\n\nEach PutRecordBatch request supports up to 500 records. Each record in the\nrequest can be as large as 1,000 KB (before 64-bit encoding), up to a limit of 4\nMB for the entire request. These limits cannot be changed.\n\nYou must specify the name of the delivery stream and the data record when using \nPutRecord. The data record consists of a data blob that can be up to 1,000 KB in\nsize, and any kind of data. For example, it could be a segment from a log file,\ngeographic location data, website clickstream data, and so on.\n\nKinesis Data Firehose buffers records before delivering them to the destination.\nTo disambiguate the data blobs at the destination, a common solution is to use\ndelimiters in the data, such as a newline (\\n) or some other character unique\nwithin the data. This allows the consumer application to parse individual data\nitems when reading the data from the destination.\n\nThe PutRecordBatch response includes a count of failed records, FailedPutCount,\nand an array of responses, RequestResponses. Even if the PutRecordBatch call\nsucceeds, the value of FailedPutCount may be greater than 0, indicating that\nthere are records for which the operation didn't succeed. Each entry in the \nRequestResponses array provides additional information about the processed\nrecord. It directly correlates with a record in the request array using the same\nordering, from the top to the bottom. The response array always includes the\nsame number of records as the request array. RequestResponses includes both\nsuccessfully and unsuccessfully processed records. Kinesis Data Firehose tries\nto process all records in each PutRecordBatch request. A single record failure\ndoes not stop the processing of subsequent records. \n\nA successfully processed record includes a RecordId value, which is unique for\nthe record. An unsuccessfully processed record includes ErrorCode and \nErrorMessage values. ErrorCode reflects the type of error, and is one of the\nfollowing values: ServiceUnavailableException or InternalFailure. ErrorMessage \nprovides more detailed information about the error.\n\nIf there is an internal server error or a timeout, the write might have\ncompleted or it might have failed. If FailedPutCount is greater than 0, retry\nthe request, resending only those records that might have failed processing.\nThis minimizes the possible duplicate records and also reduces the total bytes\nsent (and corresponding charges). We recommend that you handle any duplicates at\nthe destination.\n\nIf PutRecordBatch throws ServiceUnavailableException, back off and retry. If the\nexception persists, it is possible that the throughput limits have been exceeded\nfor the delivery stream.\n\nData records sent to Kinesis Data Firehose are stored for 24 hours from the time\nthey are added to a delivery stream as it attempts to send the records to the\ndestination. If the destination is unreachable for more than 24 hours, the data\nis no longer available.\n\nDon't concatenate two or more base64 strings to form the data fields of your\nrecords. Instead, concatenate the raw data, then perform base64 encoding.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/PutRecordBatch.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartDeliveryStreamEncryption": {
            "main": "./lib/actions/StartDeliveryStreamEncryption.js",
            "title": "StartDeliveryStreamEncryption",
            "description": "Enables server-side encryption (SSE) for the delivery stream. \n\nThis operation is asynchronous. It returns immediately. When you invoke it,\nKinesis Data Firehose first sets the status of the stream to ENABLING, and then\nto ENABLED. You can continue to read and write data to your stream while its\nstatus is ENABLING, but the data is not encrypted. It can take up to 5 seconds\nafter the encryption status changes to ENABLED before all records written to the\ndelivery stream are encrypted. To find out whether a record or a batch of\nrecords was encrypted, check the response elements PutRecordOutput$Encrypted and \nPutRecordBatchOutput$Encrypted, respectively.\n\nTo check the encryption state of a delivery stream, use DescribeDeliveryStream.\n\nYou can only enable SSE for a delivery stream that uses DirectPut as its source. \n\nThe StartDeliveryStreamEncryption and StopDeliveryStreamEncryption operations\nhave a combined limit of 25 calls per delivery stream per 24 hours. For example,\nyou reach the limit if you call StartDeliveryStreamEncryption 13 times and \nStopDeliveryStreamEncryption 12 times for the same delivery stream in a 24-hour\nperiod.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartDeliveryStreamEncryption.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StopDeliveryStreamEncryption": {
            "main": "./lib/actions/StopDeliveryStreamEncryption.js",
            "title": "StopDeliveryStreamEncryption",
            "description": "Disables server-side encryption (SSE) for the delivery stream. \n\nThis operation is asynchronous. It returns immediately. When you invoke it,\nKinesis Data Firehose first sets the status of the stream to DISABLING, and then\nto DISABLED. You can continue to read and write data to your stream while its\nstatus is DISABLING. It can take up to 5 seconds after the encryption status\nchanges to DISABLED before all records written to the delivery stream are no\nlonger subject to encryption. To find out whether a record or a batch of records\nwas encrypted, check the response elements PutRecordOutput$Encrypted and \nPutRecordBatchOutput$Encrypted, respectively.\n\nTo check the encryption state of a delivery stream, use DescribeDeliveryStream. \n\nThe StartDeliveryStreamEncryption and StopDeliveryStreamEncryption operations\nhave a combined limit of 25 calls per delivery stream per 24 hours. For example,\nyou reach the limit if you call StartDeliveryStreamEncryption 13 times and \nStopDeliveryStreamEncryption 12 times for the same delivery stream in a 24-hour\nperiod.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StopDeliveryStreamEncryption.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "TagDeliveryStream": {
            "main": "./lib/actions/TagDeliveryStream.js",
            "title": "TagDeliveryStream",
            "description": "Adds or updates tags for the specified delivery stream. A tag is a key-value\npair that you can define and assign to AWS resources. If you specify a tag that\nalready exists, the tag value is replaced with the value that you specify in the\nrequest. Tags are metadata. For example, you can add friendly names and\ndescriptions or other types of information that can help you distinguish the\ndelivery stream. For more information about tags, see Using Cost Allocation Tags\n[https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html] \nin the AWS Billing and Cost Management User Guide. \n\nEach delivery stream can have up to 50 tags. \n\nThis operation has a limit of five transactions per second per account.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/TagDeliveryStream.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "UntagDeliveryStream": {
            "main": "./lib/actions/UntagDeliveryStream.js",
            "title": "UntagDeliveryStream",
            "description": "Removes tags from the specified delivery stream. Removed tags are deleted, and\nyou can't recover them after this operation successfully completes.\n\nIf you specify a tag that doesn't exist, the operation ignores it.\n\nThis operation has a limit of five transactions per second per account.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/UntagDeliveryStream.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "UpdateDestination": {
            "main": "./lib/actions/UpdateDestination.js",
            "title": "UpdateDestination",
            "description": "Updates the specified destination of the specified delivery stream.\n\nUse this operation to change the destination type (for example, to replace the\nAmazon S3 destination with Amazon Redshift) or change the parameters associated\nwith a destination (for example, to change the bucket name of the Amazon S3\ndestination). The update might not occur immediately. The target delivery stream\nremains active while the configurations are updated, so data writes to the\ndelivery stream can continue during this process. The updated configurations are\nusually effective within a few minutes.\n\nSwitching between Amazon ES and other services is not supported. For an Amazon\nES destination, you can only update to another Amazon ES destination.\n\nIf the destination type is the same, Kinesis Data Firehose merges the\nconfiguration parameters specified with the destination configuration that\nalready exists on the delivery stream. If any of the parameters are not\nspecified in the call, the existing values are retained. For example, in the\nAmazon S3 destination, if EncryptionConfiguration is not specified, then the\nexisting EncryptionConfiguration is maintained on the destination.\n\nIf the destination type is not the same, for example, changing the destination\nfrom Amazon S3 to Amazon Redshift, Kinesis Data Firehose does not merge any\nparameters. In this case, all parameters must be specified.\n\nKinesis Data Firehose uses CurrentDeliveryStreamVersionId to avoid race\nconditions and conflicting merges. This is a required field, and the service\nupdates the configuration only if the existing configuration has a version ID\nthat matches. After the update is applied successfully, the version ID is\nupdated, and can be retrieved using DescribeDeliveryStream. Use the new version\nID to set CurrentDeliveryStreamVersionId in the next call.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/UpdateDestination.in.json",
                "out": {
                    "type": "object"
                }
            }
        }
    }
}